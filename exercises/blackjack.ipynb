{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict\n",
    "from enum import Enum\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blackjack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cards to be used in Blackjack are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Card(Enum):\n",
    "    ACE = 1\n",
    "    TWO = 2\n",
    "    THREE = 3\n",
    "    FOUR = 4\n",
    "    FIVE = 5\n",
    "    SIX = 6\n",
    "    SEVEN = 7\n",
    "    EIGHT = 8\n",
    "    NINE = 9\n",
    "    FACE = 10   # This groups 10, Jack, Queen and King whose value is the same\n",
    "\n",
    "card_values = {\n",
    "    Card.ACE: [1, 11],\n",
    "    Card.TWO: 2,\n",
    "    Card.THREE: 3,\n",
    "    Card.FOUR: 4,\n",
    "    Card.FIVE: 5,\n",
    "    Card.SIX: 6,\n",
    "    Card.SEVEN: 7,\n",
    "    Card.EIGHT: 8,\n",
    "    Card.NINE: 9,\n",
    "    Card.FACE: 10   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the player and the dealer are given 2 cards. The first one is shown, the second is face down. Then, the player starts requesting cards and what matters is the sum of the values. Therefore, a state in our system is made by:\n",
    "- The current sum of cards of the player. Note that below 11 we don't need to make decisions: we simply draw the next card. So this sum is valuable between 12 and 21.\n",
    "- The card shown by the dealer. It can be an ace or any number up to 10.\n",
    "\n",
    "We also add an indicator for **usable aces**, which are aces the player can use as a 11 without going bust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "states = [{\n",
    "        'player_sum': player_sum, \n",
    "        'dealer_card': dealer_card, \n",
    "        'usable_ace': usable_ace\n",
    "    }\n",
    "    for player_sum in range(12,21+1)\n",
    "    for dealer_card in Card\n",
    "    for usable_ace in [True, False]\n",
    "]\n",
    "\n",
    "print(len(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Player's actions are to hit or stick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    HIT = 0\n",
    "    STICK = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy for the player is to only stick at 20 or 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_policy = {\n",
    "    (s['player_sum'], s['dealer_card'], s['usable_ace']): {\n",
    "        Action.HIT: 1*(s['player_sum'] < 18),\n",
    "        Action.STICK: 1*(s['player_sum'] >= 18)\n",
    "    }\n",
    "    for s in states\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for playing an episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final comparison...\n",
      "Player cards: [<Card.NINE: 9>, <Card.FACE: 10>]\n",
      "Dealer cards: [<Card.TWO: 2>, <Card.SEVEN: 7>, <Card.FACE: 10>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(19, <Card.TWO: 2>, False)], [<Action.STICK: 1>], [-1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_action_according_to_policy(current_state: Tuple[int,int], \n",
    "        policy:Dict[Tuple[int,int], Dict[Action, float]])-> Action:\n",
    "    max_val = max(policy[current_state].values())\n",
    "    # Randomly break ties\n",
    "    keys = [key for key, value in policy[current_state].items() if value == max_val]\n",
    "    choice = np.random.choice(keys)\n",
    "    return choice\n",
    "\n",
    "def select_random_action(current_state: Tuple[int,int], \n",
    "        policy:Dict[Tuple[int,int], Dict[Action, float]])-> Action:\n",
    "    return np.random.choice(list(policy[current_state].keys()))\n",
    "\n",
    "def deal_cards(num=1):\n",
    "    return list(np.random.choice(Card, num)) if num > 1 else np.random.choice(Card, num)[0]\n",
    "\n",
    "def get_value(cards, aces_as_11):\n",
    "    total_sum = 0\n",
    "    for i, c in enumerate(cards):\n",
    "        if c == Card.ACE:\n",
    "            if len(aces_as_11) > 0 and i in aces_as_11:\n",
    "                total_sum += 11\n",
    "            else:\n",
    "                total_sum += 1\n",
    "        else:\n",
    "            total_sum += card_values[c]\n",
    "    return total_sum\n",
    "\n",
    "def play_episode(policy:Dict[Tuple[int, int], Dict[Action,float]], epsilon=0.1, verbose=False):\n",
    "    player_cards = deal_cards(num=2)\n",
    "    dealer_cards = deal_cards(num=2)\n",
    "    player_aces_as_11 = np.where(np.array(player_cards) == Card.ACE)[0]\n",
    "    dealer_aces_as_11 = np.where(np.array(dealer_cards) == Card.ACE)[0]\n",
    "    # Note: if there are two aces, only the first one counts as 11\n",
    "    if len(player_aces_as_11) > 1:\n",
    "        player_aces_as_11 = [player_aces_as_11[0]]\n",
    "    if len(dealer_aces_as_11) > 1:\n",
    "        dealer_aces_as_11 = [dealer_aces_as_11[0]]\n",
    "    dealer_visible = dealer_cards[0]\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    # If the player has 21, it has won unless the dealer also has 21\n",
    "    player_sum = get_value(player_cards, aces_as_11=player_aces_as_11)\n",
    "    if player_sum == 21:\n",
    "        states.append((21, dealer_visible, True))\n",
    "        actions.append(Action.STICK)\n",
    "        if not get_value(dealer_cards, aces_as_11=dealer_aces_as_11):\n",
    "            rewards.append(-1)\n",
    "        else:\n",
    "            rewards.append(1)\n",
    "        if verbose:\n",
    "            print(\"Someone has 21 in two cards\")\n",
    "            print(\"Player cards: {}\".format(player_cards))\n",
    "            print(\"Dealer cards: {}\".format(dealer_cards))\n",
    "        return states, actions, rewards\n",
    "    # If the player has less than 12, he hits until he reaches it.\n",
    "    while player_sum < 12:\n",
    "        new_card = deal_cards(num=1)\n",
    "        player_cards.append(new_card)\n",
    "        player_sum = get_value(player_cards, aces_as_11=player_aces_as_11)\n",
    "    # At this point, the player has >= 12. We can let the agent start making choices.\n",
    "    current_state = (player_sum, dealer_visible, len(player_aces_as_11) > 0)\n",
    "    states.append(current_state)\n",
    "    action = None\n",
    "    while not action == Action.STICK:\n",
    "        # Epsilon-greedy selection\n",
    "        if np.random.random() <= epsilon:\n",
    "            action = select_random_action(current_state, policy)\n",
    "        else:\n",
    "            action = select_action_according_to_policy(current_state, policy)\n",
    "        actions.append(action)\n",
    "        # If the action is to hit, get a new card\n",
    "        if action == Action.HIT:\n",
    "            new_card = deal_cards(num=1)\n",
    "            player_cards.append(new_card)\n",
    "            player_sum = get_value(player_cards, \n",
    "                aces_as_11=player_aces_as_11\n",
    "            )\n",
    "            current_state = (player_sum, dealer_visible, len(player_aces_as_11) > 0)\n",
    "            # To get the reward, check what is the player's sum at this point to see if we have lost:\n",
    "            if player_sum > 21:\n",
    "                # Player has lost\n",
    "                rewards.append(-1)\n",
    "                if verbose:\n",
    "                    print(\"Player has gone bust!!\")\n",
    "                    print(\"Player cards: {}\".format(player_cards))\n",
    "                    print(\"Dealer cards: {}\".format(dealer_cards))\n",
    "                return states, actions, rewards\n",
    "            else:\n",
    "                # Go on with the choices\n",
    "                states.append(current_state)\n",
    "                rewards.append(0)\n",
    "    # Here we have chosen to stick and not have bust\n",
    "    # Now it's the dealer's turn. The dealer always stops when he goes over 17.\n",
    "    # He wins if he has 21 with two cards, or if his score goes over the player's.\n",
    "    # He loses if he stops before the player's score.\n",
    "    dealer_score = get_value(dealer_cards,\n",
    "        aces_as_11=dealer_aces_as_11\n",
    "    )\n",
    "    if dealer_score == 21:\n",
    "        # Player has lost\n",
    "        rewards.append(-1)\n",
    "        return states, actions, rewards\n",
    "    else:\n",
    "        while dealer_score < 17:\n",
    "            new_card = deal_cards(num=1)\n",
    "            dealer_cards.append(new_card)\n",
    "            dealer_score = get_value(dealer_cards,\n",
    "                aces_as_11=dealer_aces_as_11\n",
    "            )\n",
    "            # Check if the new card has made the dealer bust\n",
    "            if dealer_score > 21:\n",
    "                # Player has won\n",
    "                rewards.append(1)\n",
    "                if verbose:\n",
    "                    print(\"Dealer has bust\")\n",
    "                    print(\"Player cards: {}\".format(player_cards))\n",
    "                    print(\"Dealer cards: {}\".format(dealer_cards))\n",
    "                return states, actions, rewards\n",
    "        # Here the dealer has not bust and has decided to stick. We need to compare\n",
    "        # his score with the player's score.\n",
    "        if verbose:\n",
    "            print(\"Final comparison...\")\n",
    "            print(\"Player cards: {}\".format(player_cards))\n",
    "            print(\"Dealer cards: {}\".format(dealer_cards))\n",
    "        if dealer_score >= player_sum:\n",
    "            # The player has lost\n",
    "            rewards.append(-1)\n",
    "            return states, actions, rewards\n",
    "        else:\n",
    "            # The player has won\n",
    "            rewards.append(1)\n",
    "            return states, actions, rewards\n",
    "\n",
    "play_episode(player_policy, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the Q value for each state-action pair. We can do that using a first-visit Monte Carlo method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_MC_Q(policy:Dict[Tuple[int, int], Dict[Action,float]], \n",
    "        episodes_to_play:int=1000, discount=0.9):\n",
    "    state_action_pairs = set(product(policy.keys(), Action))\n",
    "    Q = {st: np.random.random() for st in state_action_pairs}\n",
    "    Returns = {st: [] for st in state_action_pairs}\n",
    "    for i in range(episodes_to_play):\n",
    "        sts, acts, rews = play_episode(policy)\n",
    "        cumulative_reward = 0\n",
    "        # For each state-action pair\n",
    "        for step in range(len(rews)-1, 0, -1):\n",
    "            cumulative_reward = discount*cumulative_reward + rews[step]\n",
    "            state_action = (sts[step], acts[step])\n",
    "            if not state_action in [(sts[s], acts[s]) for s in range(step)]:\n",
    "                Returns[state_action].append(cumulative_reward)     # The cumulative reward is appended only once for the first visit method\n",
    "                Q[state_action] = np.mean(Returns[state_action])    # Compute average of rewards for Q\n",
    "    return Q\n",
    "\n",
    "Q = first_visit_MC_Q(player_policy, episodes_to_play=100000, discount=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If q* is well approximated by Q, an agent that always follows the action whose Q value is highest should behave optimally (or at least, be quite good in general)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_agent_policy = {\n",
    "    s : {\n",
    "        Action.HIT: Q[(s, Action.HIT)],\n",
    "        Action.STICK: Q[(s, Action.STICK)]\n",
    "    }\n",
    "    for s in player_policy.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play some games!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player has gone bust!!\n",
      "Player cards: [<Card.TWO: 2>, <Card.THREE: 3>, <Card.EIGHT: 8>, <Card.THREE: 3>, <Card.SEVEN: 7>]\n",
      "Dealer cards: [<Card.EIGHT: 8>, <Card.SEVEN: 7>]\n",
      "\n",
      "Final comparison...\n",
      "Player cards: [<Card.ACE: 1>, <Card.SEVEN: 7>]\n",
      "Dealer cards: [<Card.FACE: 10>, <Card.SIX: 6>, <Card.TWO: 2>]\n",
      "\n",
      "Dealer has bust\n",
      "Player cards: [<Card.THREE: 3>, <Card.FACE: 10>]\n",
      "Dealer cards: [<Card.SEVEN: 7>, <Card.NINE: 9>, <Card.EIGHT: 8>]\n",
      "\n",
      "\n",
      "Final comparison...\n",
      "Player cards: [<Card.SEVEN: 7>, <Card.SEVEN: 7>, <Card.FIVE: 5>]\n",
      "Dealer cards: [<Card.SEVEN: 7>, <Card.FACE: 10>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    play_episode(new_agent_policy, verbose=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the Monte Carlo strategy to update the strategy as we go along, making the agent greedy on the Q value it's computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_MC_Q_update(initial_policy:Dict[Tuple[int, int], Dict[Action,float]], \n",
    "        episodes_to_play:int=1000, discount=0.9):\n",
    "    state_action_pairs = set(product(initial_policy.keys(), Action))\n",
    "    Q = {st: np.random.random() for st in state_action_pairs}\n",
    "    Returns = {st: [] for st in state_action_pairs}\n",
    "    policy = initial_policy\n",
    "    for i in range(episodes_to_play):\n",
    "        sts, acts, rews = play_episode(policy)\n",
    "        cumulative_reward = 0\n",
    "        # For each state-action pair\n",
    "        for step in range(len(rews)-1, 0, -1):\n",
    "            cumulative_reward = discount*cumulative_reward + rews[step]\n",
    "            state_action = (sts[step], acts[step])\n",
    "            if not state_action in [(sts[s], acts[s]) for s in range(step)]:\n",
    "                Returns[state_action].append(cumulative_reward)     # The cumulative reward is appended only once for the first visit method\n",
    "                Q[state_action] = np.mean(Returns[state_action])    # Compute average of rewards for Q\n",
    "                # UPDATE THE POLICY by making it greedy based on the Q value\n",
    "                policy = {\n",
    "                    s : {\n",
    "                        Action.HIT: 1 if Q[(s, Action.HIT)] >= Q[(s, Action.STICK)] else 0,\n",
    "                        Action.STICK: 1 if Q[(s, Action.STICK)] > Q[(s, Action.HIT)] else 0\n",
    "                    }\n",
    "                    for s in policy.keys()\n",
    "                }\n",
    "    return Q\n",
    "\n",
    "Q = first_visit_MC_Q(player_policy, episodes_to_play=100000, discount=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play some more games with this updated policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player has gone bust!!\n",
      "Player cards: [<Card.SIX: 6>, <Card.ACE: 1>, <Card.SIX: 6>]\n",
      "Dealer cards: [<Card.SIX: 6>, <Card.ACE: 1>]\n",
      "\n",
      "Dealer has bust\n",
      "Player cards: [<Card.FIVE: 5>, <Card.SEVEN: 7>, <Card.NINE: 9>]\n",
      "Dealer cards: [<Card.NINE: 9>, <Card.FIVE: 5>, <Card.TWO: 2>, <Card.FACE: 10>]\n",
      "\n",
      "Dealer has bust\n",
      "Player cards: [<Card.NINE: 9>, <Card.ACE: 1>]\n",
      "Dealer cards: [<Card.THREE: 3>, <Card.FOUR: 4>, <Card.EIGHT: 8>, <Card.EIGHT: 8>]\n",
      "\n",
      "Player has gone bust!!\n",
      "Player cards: [<Card.ACE: 1>, <Card.SIX: 6>, <Card.NINE: 9>]\n",
      "Dealer cards: [<Card.FOUR: 4>, <Card.FOUR: 4>]\n",
      "\n",
      "Final comparison...\n",
      "Player cards: [<Card.SIX: 6>, <Card.THREE: 3>, <Card.SEVEN: 7>]\n",
      "Dealer cards: [<Card.TWO: 2>, <Card.FOUR: 4>, <Card.THREE: 3>, <Card.EIGHT: 8>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    play_episode(new_agent_policy, verbose=True)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac7a45bdc5a3df71ccc95c77502ad70b449194eee2e6719931ab0f9b60e7f47c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
